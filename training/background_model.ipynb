{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path of the project root, which is one level above the directory this script is in\n",
    "project_root = os.path.abspath('..')\n",
    "sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pokemon import *\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn import MSELoss\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataConfig:\n",
    "    image_dir = \"train_images\"\n",
    "    annotations_path = \"train_images/annotations.json\"\n",
    "    training_data_path = \"data/background_training_data.pkl\"\n",
    "    validation_data_path = \"data/background_val_data.pkl\"\n",
    "    test_image_dir = \"test_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# TRANSFORMATIONS = 200\n",
    "# BACKGROUND_IMAGE_INDEXES = [23, 30, 32, 35, 38, 43, 44, 48, 51, 54, 64, 70, 74, 80, 85, 93, 109, 115, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
    "# background_image_names = [f\"Image {i}.jpg\" for i in BACKGROUND_IMAGE_INDEXES] + [f\"Image_{i}.jpg\" for i in BACKGROUND_IMAGE_INDEXES]\n",
    "\n",
    "# data = load_image_data(DataConfig.image_dir, DataConfig.annotations_path)\n",
    "# data = [i for i in data if i.image_name in background_image_names]\n",
    "# val_data = random.sample(data, 5)\n",
    "# train_data = [item for item in data if item not in val_data]\n",
    "\n",
    "# def affine_transform_data(data):\n",
    "#     return [augment_data(i) for i in data]\n",
    "\n",
    "# train_dataset = []\n",
    "# # Create a thread pool\n",
    "# with ThreadPoolExecutor() as executor:\n",
    "#     for transformed_data in tqdm(executor.map(affine_transform_data, [train_data]*TRANSFORMATIONS), total=TRANSFORMATIONS):\n",
    "#         training_data = [\n",
    "#             (i.resized_image.float(), torch.tensor(i.resized_annotation).flatten().float())\n",
    "#             for i in transformed_data\n",
    "#         ]\n",
    "#         train_dataset.extend(training_data)\n",
    "\n",
    "# train_dataset += [\n",
    "#     (i.resized_image.float(), torch.tensor(i.resized_annotation).flatten().float()) for i in train_data\n",
    "# ]\n",
    "\n",
    "# with open(DataConfig.training_data_path, \"wb\") as f:\n",
    "#     pickle.dump(train_dataset, f)\n",
    "\n",
    "# val_dataset = []\n",
    "# # Create a thread pool\n",
    "# with ThreadPoolExecutor() as executor:\n",
    "#     for transformed_data in tqdm(executor.map(affine_transform_data, [val_data]*TRANSFORMATIONS), total=TRANSFORMATIONS):\n",
    "#         training_data = [\n",
    "#             (i.resized_image.float(), torch.tensor(i.resized_annotation).flatten().float())\n",
    "#             for i in transformed_data\n",
    "#         ]\n",
    "#         val_dataset.extend(training_data)\n",
    "\n",
    "# val_dataset += [\n",
    "#     (i.resized_image.float(), torch.tensor(i.resized_annotation).flatten().float()) for i in val_data\n",
    "# ]\n",
    "\n",
    "# with open(DataConfig.validation_data_path, \"wb\") as f:\n",
    "#     pickle.dump(val_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the pickle file\n",
    "with open(DataConfig.training_data_path, \"rb\") as f:\n",
    "    train_dataset = pickle.load(f)\n",
    "\n",
    "with open(DataConfig.validation_data_path, \"rb\") as f:\n",
    "    val_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    model_name = \"background_hrnet\"\n",
    "    save_epochs = 5\n",
    "    final_layer_epochs = 10\n",
    "    full_model_epochs = 5\n",
    "    checkpoint_dir = \"model_checkpoints\"\n",
    "    final_layer_learning_rate = 0.01\n",
    "    full_model_learning_rate = 0.001\n",
    "    batch_size = 32\n",
    "    weight_decay = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PokemonData(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=ModelConfig.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = PokemonData(val_dataset)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=ModelConfig.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(ModelConfig.model_name)\n",
    "loss_fn = MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Phase 1: Train only the final layer\n",
    "# for name, param in model.named_parameters():\n",
    "#     if \"classifier\" not in name:\n",
    "#         param.requires_grad = False\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=ModelConfig.final_layer_learning_rate, weight_decay=ModelConfig.weight_decay)\n",
    "# model, optimizer = load_latest_checkpoint(ModelConfig.checkpoint_dir, ModelConfig.model_name, model)\n",
    "\n",
    "# train_model(\n",
    "#     model=model,\n",
    "#     train_dataloader=train_dataloader,\n",
    "#     val_dataloader=validation_dataloader,\n",
    "#     optimizer=optimizer,\n",
    "#     loss_fn=loss_fn,\n",
    "#     num_epochs=ModelConfig.final_layer_epochs,\n",
    "#     is_final_layer_only=True,\n",
    "#     save_epochs=ModelConfig.save_epochs,\n",
    "#     checkpoint_dir=ModelConfig.checkpoint_dir,\n",
    "#     model_name=ModelConfig.model_name\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Train the entire model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=ModelConfig.full_model_learning_rate, weight_decay=ModelConfig.weight_decay)\n",
    "model, optimizer = load_latest_checkpoint(ModelConfig.checkpoint_dir, ModelConfig.model_name, model, optimizer)\n",
    "for group in optimizer.param_groups:\n",
    "    group['weight_decay'] = ModelConfig.weight_decay\n",
    "\n",
    "train_model(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=validation_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    num_epochs=ModelConfig.full_model_epochs,\n",
    "    is_final_layer_only=False,\n",
    "    save_epochs=ModelConfig.save_epochs,\n",
    "    checkpoint_dir=ModelConfig.checkpoint_dir,\n",
    "    model_name=ModelConfig.model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(ModelConfig.model_name)\n",
    "model = load_latest_checkpoint(ModelConfig.checkpoint_dir, ModelConfig.model_name, model)\n",
    "model.eval();\n",
    "model.to(\"cpu\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_test_image_data(DataConfig.test_image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data:\n",
    "    i.predict_annotations(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('test_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c80c09c1b298b3abc58855546266a88101147539d25dab50d705a2cf327760dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
